// save RDD back to HDFS

val orders = sc.textFile("retail_db/orders")

val orderCountByStatus = orders.map(order => (order.split(",")(3), 1)).reduceByKey((total,element) => total+element)
orderCountByStatus: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[3] at reduceByKey at <console>:29

scala> orderCountByStatus.take(10).foreach(println)
(PENDING_PAYMENT,15030)                                                         
(CLOSED,7556)
(CANCELED,1428)
(PAYMENT_REVIEW,729)
(PENDING,7610)
(ON_HOLD,3798)
(PROCESSING,8275)
(SUSPECTED_FRAUD,1558)
(COMPLETE,22899)

scala> orderCountByStatus.saveAsTextFile("cloudera/data-master/order_count_by_status")


[cloudera@quickstart ~]$ hadoop fs -rm -R cloudera/data-master/order_count_by_status

orderCountByStatus.map(rec => rec._1 +"\t" +rec._2).saveAsTextFile("cloudera/data-master/order_count_by_status")

 sc.textFile("cloudera/data-master/order_count_by_status").collect.foreach(println)
PENDING_PAYMENT	15030
CLOSED	7556
CANCELED	1428
PAYMENT_REVIEW	729
PENDING	7610
ON_HOLD	3798
PROCESSING	8275
SUSPECTED_FRAUD	1558
COMPLETE	22899

//compression textfile hdfs

orderCountByStatus.saveAsTextFile("cloudera/data-master/order_count_by_status_snappy",classof[ org.apache.hadoop.io.compress.SnappyCodec])

sc.textFile('cloudera/data-master/order_count_by_status_snappy").collect.foreach(println)

//save data in different file formatsal 

val ordersDF = sqlContext.read.json("retail_db_json/orders")
 ordersDF.save("cloudera/data-master/orders_parquet", "parquet")

scala> sqlContext.load("cloudera/data-master/orders_parquet","parquet").show

 ordersDF.write.orc("cloudera/data-master/orders_orc")

sqlContext.load("cloudera/data-master/orders_orc", "orc").show

sqlContext.read.orc("cloudera/data-master/orders_orc").show